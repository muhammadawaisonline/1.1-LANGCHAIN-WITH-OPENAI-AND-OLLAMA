{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x25e4e24fdd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader =WebBaseLoader(\"https://docs.smith.langchain.com/evaluation/concepts\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConcepts | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityEvaluationConceptual GuideHow-to GuidesDatasetsEvaluationHuman feedbackTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationOn this pageConcepts\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.\\nLangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let\\'s introduce the core components of LangSmith evaluation:\\n\\nDataset: These are the inputs to your application used for conducting evaluations.\\nEvaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset.\\n\\n\\nDatasets\\u200b\\nDatasets are the cornerstone of the LangSmith evaluation workflow. They consist of examples that provide inputs and, optionally, expected reference outputs for assessing your AI application. Each example in a dataset is a data point with an inputs dictionary, an optional outputs dictionary, and an optional metadata dictionary. The outputs dictionary, if present, contains information for your evaluator(s) that remains hidden from the function being evaluated. It may include a reference key (expected output or label, or additional context), example-specific criteria, information the function must_include and must_exclude for that particular example, or any other information to check your system\\'s outputs.\\nCreating datasets\\u200b\\nThere are various ways to build datasets for evaluation, including:\\nManually curated examples\\nThis is how we typically recommend people get started creating datasets.\\nFrom building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,\\nand what \"good\" responses may be.\\nYou probably want to cover a few different common edge cases or situations you can imagine.\\nEven 10-20 high-quality, manually-curated examples can go a long way.\\nHistorical logs\\nOnce you have an application in production, you start getting valuable information: how are users actually using it?\\nThis information can be valuable to capture and store in datasets. This allows you to test against these\\nuse cases as you iterate on your application.\\nIf your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?\\nThere are a few heuristics you can follow.\\nIf possible - try to collect end user feedback. You can then see which datapoints got negative feedback.\\nThat is super valuable! These are spots where your application did not perform well.\\nYou should add these to your dataset to test against in the future. You can also use other heuristics\\nto identify \"interesting\" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset.\\nSynthetic data\\nOnce you have a few examples, you can try to artificially generate examples.\\nIt\\'s generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.\\nThis can be a useful way to get a lot of datapoints, quickly.\\ntipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:\\nSee our video on Manually curated datasets.\\nSee our videos on Datasets from traces\\n\\nTypes of datasets\\u200b\\nLangSmith offers three distinct dataset types:\\n\\n\\nkv (Key-Value) Dataset:\\n\\n\"Inputs\" and \"outputs\" are represented as arbitrary key-value pairs.\\nThe kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.\\nThis dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.\\n\\n\\n\\nllm (Language Model) Dataset:\\n\\nThe llm dataset is designed for evaluating \"completion\" style language models.\\nThe \"inputs\" dictionary contains a single \"input\" key mapped to the prompt string.\\nThe \"outputs\" dictionary contains a single \"output\" key mapped to the corresponding response string.\\nThis dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.\\n\\n\\n\\nchat Dataset:\\n\\nThe chat dataset is designed for evaluating LLM structured \"chat\" messages as inputs and outputs.\\nThe \"inputs\" dictionary contains a single \"input\" key mapped to a list of serialized chat messages\\nThe \"outputs\" dictionary contains a single \"output\" key mapped to a list of serialized chat messages.\\nThis dataset type is useful for evaluating conversational AI systems or chatbots.\\n\\n\\n\\nPartitioning datasets\\u200b\\nWhen setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.\\ntipTo learn more about creating dataset splits in LangSmith:\\nSee our video on dataset splits in the LangSmith Evaluation series.\\nSee our documentation here.\\n\\nEvaluators\\u200b\\nEvaluators are functions in LangSmith that score how well your application performs on a particular example.\\nEvaluators receive these inputs:\\n\\nExample: The example from your Dataset.\\nRoot_run: The output and intermediate steps from running the inputs through the application.\\n\\nThe evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of:\\n\\nKey: The name of the metric being evaluated.\\nScore: The value of the metric for this example.\\nComment: The reasoning or additional string information justifying the score.\\n\\nThere are a few approaches and types of scoring functions that can be used in LangSmith evaluation.\\nHuman\\u200b\\nHuman evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).\\ntipSee our video using LangSmith to capture human feedback for prompt engineering.\\nHeuristic\\u200b\\nHeuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly).\\ntipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).\\nWatch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.\\nRead our documentation on custom evaluators.\\nSee our blog using custom evaluators for code generation.\\n\\nLLM-as-judge\\u200b\\nLLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference).\\ntipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series.\\nWith LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores.\\ntipSee documentation on our workflow to audit and manually correct evaluator scores here.\\nPairwise\\u200b\\nPairwise evaluators pick the better of two task outputs based upon some criteria.\\nThis can use either a heuristic (\"which response is longer\"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).\\nWhen should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.\\nThis can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it\\'s better than a baseline.\\nApplying evaluations\\u200b\\nWe can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference.\\n\\nEach time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application\\'s performance over time. Additionally, you can compare multiple experiments in a comparison view.\\n\\nIn the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we\\'ll discuss a few common paradigms for offline evaluation.\\nUnit Tests\\u200b\\nUnit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.\\nThis means they often use heuristics to evaluate.\\nYou generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).\\nThese types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)\\nto cache LLM calls (because those can quickly rack up!).\\ntipTo learn more about unit tests with LangSmith, check out our unit testing video.\\nRegression Testing\\u200b\\nRegression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.\\nThey are also commonly done when evaluating new or different models.\\ntip\\nTo learn more about regression testing with LangSmith, see our regression testing video\\nSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video.\\n\\nLangSmith\\'s comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):\\n\\nBack-testing\\u200b\\nBack-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.\\nThis is commonly used to evaluate new model versions.\\nAnthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.\\nThen compare those results to what actually happened in production.\\ntipSee our video on Back-testing to learn about this workflow.\\nPairwise-testing\\u200b\\nIt can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators.\\ntip\\nWatch the Pairwise evaluation video in our LangSmith Evaluation series.\\nSee our blog post on pairwise evaluation.\\n\\nOnline Evaluation\\u200b\\nWhereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation.\\ntipExplore our videos on online evaluation:\\nOnline evaluation in our LangSmith Evaluation series\\nOnline evaluation with focus on guardrails in our LangSmith Evaluation series\\nOnline evaluation with focus on RAG in our LangSmith Evaluation series\\n\\nExperiment Configurations\\u200b\\nLangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below.\\ntipSee documentation on using evaluate here.\\nRepetitions\\u200b\\nOne of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application\\'s performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI.\\ntip\\nSee the video on Repetitions in our LangSmith Evaluation series\\nSee our documentation on Repetitions\\n\\n\\nEvaluating Specific LLM Applications\\nBelow, we will discuss evaluation of a few specific, popular LLM applications.\\nAgents\\u200b\\nLLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required.\\n\\nBelow is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response.\\n\\nThis sets up three general types of agent evaluations that users are often interested in:\\n\\nFinal Response: Evaluate the agent\\'s final response.\\nSingle step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).\\nTrajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.\\n\\n\\nBelow we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.\\nNote that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!\\nEvaluating an agent\\'s final response\\u200b\\nOne way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done.\\nThe inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don\\'t need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time.\\nThe output should be the agent\\'s final response.\\nThe evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response.\\nHowever, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics.\\ntipSee our tutorial on evaluating agent response.\\nEvaluating a single step of an agent\\u200b\\nAgents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do.\\nThe inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps.\\nThe outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next.\\nThe evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string.\\nThere are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristic evaluation of the selected tool relative to the reference tool. One downside is that they don\\'t capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent\\'s trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses).\\ntipSee our tutorial on evaluating a single step of an agent.\\nEvaluating an agent\\'s trajectory\\u200b\\nEvaluating an agent\\'s trajectory involves looking at all the steps an agent took and evaluating that sequence of steps.\\nThe inputs are again the inputs to the overall agent (the user input, and optionally a list of tools).\\nThe outputs are a list of tool calls, which can be formulated as an \"exact\" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order).\\nThe evaluator here is some function over the steps taken. Assessing the \"exact\" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong.\\nTo address these flaws, evaluation metrics can focused on the number of \"incorrect\" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order.\\nHowever, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent\\'s trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with.\\ntipSee our tutorial on evaluating agent trajectory.\\nBest practices\\u200b\\nAgents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects:\\ntip\\nTest multiple tool calling LLMs with your agent.\\nIt\\'s possible that faster and / or lower cost LLMs show acceptable performance for your application.\\nTrying evaluating the agent at multiple levels - both end-to-end, as well as at particular steps\\nUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.\\nSee the video on Repetitions in our LangSmith Evaluation series\\n\\nRetrieval Augmented Generation (RAG)\\u200b\\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user\\'s input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\\nDataset\\u200b\\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\\nEvaluator\\u200b\\nLLM-as-judge is a commonly used evaluator for RAG because it\\'s an effective way to evaluate factual accuracy or consistency between texts.\\n\\nWhen evaluating RAG applications, you have two main options:\\n\\nReference answer: Compare the RAG chain\\'s generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\\nReference-free: Perform self-consistency checks using prompts that don\\'t require a reference answer (represented by orange, green, and red in the above figure).\\n\\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\\nRAG answer correctness evaluation\\nRAG answer hallucination\\nRAG document relevance\\nRAG intermediate steps evaluation\\n\\nApplying RAG Evaluation\\u200b\\nWhen applying RAG evaluation, consider the following approaches:\\n\\n\\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\\n\\n\\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application\\'s performance in real-time scenarios.\\n\\n\\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\\n\\n\\ntipExplore our LangSmith video series for more insights on RAG evaluation:\\nRAG with online evaluation\\nRAG pairwise evaluation\\n\\nRAG evaluation summary\\u200b\\nUse CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes\\nCommon use-cases\\u200b\\nSummarization\\u200b\\nSummarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.\\nDeveloper curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below.\\nLLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers.\\nOnline or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs):\\ntipSee our LangSmith video series to go deeper on these concepts:\\nVideo on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared\\n\\nUse CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes\\nClassification / Tagging\\u200b\\nClassification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below:\\nA central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc).\\nIf ground truth reference labels are provided, then it\\'s common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference).\\nOnline or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc).\\ntipSee our LangSmith video series to go deeper on these concepts:\\nOnline evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared\\n\\nUse CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNoWas this page helpful?You can leave detailed feedback on GitHub.PreviousAdd observability to your LLM applicationNextConceptual GuideDatasetsCreating datasetsTypes of datasetsPartitioning datasetsEvaluatorsHumanHeuristicLLM-as-judgePairwiseApplying evaluationsUnit TestsRegression TestingBack-testingPairwise-testingOnline EvaluationExperiment ConfigurationsAgentsEvaluating an agent\\'s final responseEvaluating a single step of an agentEvaluating an agent\\'s trajectoryBest practicesRetrieval Augmented Generation (RAG)DatasetEvaluatorApplying RAG EvaluationRAG evaluation summaryCommon use-casesSummarizationClassification / TaggingCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs= loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityEvaluationConceptual GuideHow-to GuidesDatasetsEvaluationHuman feedbackTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationOn this pageConcepts'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Dataset: These are the inputs to your application used for conducting evaluations.\\nEvaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Datasets\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Datasets are the cornerstone of the LangSmith evaluation workflow. They consist of examples that provide inputs and, optionally, expected reference outputs for assessing your AI application. Each example in a dataset is a data point with an inputs dictionary, an optional outputs dictionary, and an optional metadata dictionary. The outputs dictionary, if present, contains information for your evaluator(s) that remains hidden from the function being evaluated. It may include a reference key'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"being evaluated. It may include a reference key (expected output or label, or additional context), example-specific criteria, information the function must_include and must_exclude for that particular example, or any other information to check your system's outputs.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Creating datasets\\u200b\\nThere are various ways to build datasets for evaluation, including:\\nManually curated examples\\nThis is how we typically recommend people get started creating datasets.\\nFrom building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,\\nand what \"good\" responses may be.\\nYou probably want to cover a few different common edge cases or situations you can imagine.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Even 10-20 high-quality, manually-curated examples can go a long way.\\nHistorical logs\\nOnce you have an application in production, you start getting valuable information: how are users actually using it?\\nThis information can be valuable to capture and store in datasets. This allows you to test against these\\nuse cases as you iterate on your application.\\nIf your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='There are a few heuristics you can follow.\\nIf possible - try to collect end user feedback. You can then see which datapoints got negative feedback.\\nThat is super valuable! These are spots where your application did not perform well.\\nYou should add these to your dataset to test against in the future. You can also use other heuristics\\nto identify \"interesting\" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset.\\nSynthetic data'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"Synthetic data\\nOnce you have a few examples, you can try to artificially generate examples.\\nIt's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.\\nThis can be a useful way to get a lot of datapoints, quickly.\\ntipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:\\nSee our video on Manually curated datasets.\\nSee our videos on Datasets from traces\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Types of datasets\\u200b\\nLangSmith offers three distinct dataset types:\\n\\n\\nkv (Key-Value) Dataset:\\n\\n\"Inputs\" and \"outputs\" are represented as arbitrary key-value pairs.\\nThe kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.\\nThis dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.\\n\\n\\n\\nllm (Language Model) Dataset:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='llm (Language Model) Dataset:\\n\\nThe llm dataset is designed for evaluating \"completion\" style language models.\\nThe \"inputs\" dictionary contains a single \"input\" key mapped to the prompt string.\\nThe \"outputs\" dictionary contains a single \"output\" key mapped to the corresponding response string.\\nThis dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.\\n\\n\\n\\nchat Dataset:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='chat Dataset:\\n\\nThe chat dataset is designed for evaluating LLM structured \"chat\" messages as inputs and outputs.\\nThe \"inputs\" dictionary contains a single \"input\" key mapped to a list of serialized chat messages\\nThe \"outputs\" dictionary contains a single \"output\" key mapped to a list of serialized chat messages.\\nThis dataset type is useful for evaluating conversational AI systems or chatbots.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Partitioning datasets\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='opinion, etc) and to evaluate your application on each split separately.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='tipTo learn more about creating dataset splits in LangSmith:\\nSee our video on dataset splits in the LangSmith Evaluation series.\\nSee our documentation here.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Evaluators\\u200b\\nEvaluators are functions in LangSmith that score how well your application performs on a particular example.\\nEvaluators receive these inputs:\\n\\nExample: The example from your Dataset.\\nRoot_run: The output and intermediate steps from running the inputs through the application.\\n\\nThe evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Key: The name of the metric being evaluated.\\nScore: The value of the metric for this example.\\nComment: The reasoning or additional string information justifying the score.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='There are a few approaches and types of scoring functions that can be used in LangSmith evaluation.\\nHuman\\u200b\\nHuman evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).\\ntipSee our video using LangSmith to capture human feedback for prompt engineering.\\nHeuristic\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Heuristic\\u200b\\nHeuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly).'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).\\nWatch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.\\nRead our documentation on custom evaluators.\\nSee our blog using custom evaluators for code generation.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='LLM-as-judge\\u200b\\nLLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference).\\ntipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores.\\ntipSee documentation on our workflow to audit and manually correct evaluator scores here.\\nPairwise\\u200b\\nPairwise evaluators pick the better of two task outputs based upon some criteria.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='This can use either a heuristic (\"which response is longer\"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).\\nWhen should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.\\nThis can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it\\'s better than a baseline.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Applying evaluations\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='outputs, then the evaluator can compare the application output to the reference.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='can compare multiple experiments in a comparison view.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation.\\nUnit Tests\\u200b\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Unit Tests\\u200b\\nUnit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.\\nThis means they often use heuristics to evaluate.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='This means they often use heuristics to evaluate.\\nYou generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).\\nThese types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)\\nto cache LLM calls (because those can quickly rack up!).\\ntipTo learn more about unit tests with LangSmith, check out our unit testing video.\\nRegression Testing\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Regression Testing\\u200b\\nRegression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='They are also commonly done when evaluating new or different models.\\ntip\\nTo learn more about regression testing with LangSmith, see our regression testing video\\nSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Back-testing\\u200b\\nBack-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.\\nThis is commonly used to evaluate new model versions.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.\\nThen compare those results to what actually happened in production.\\ntipSee our video on Back-testing to learn about this workflow.\\nPairwise-testing\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Pairwise-testing\\u200b\\nIt can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators.\\ntip\\nWatch the Pairwise evaluation video in our LangSmith Evaluation series.\\nSee our blog post on pairwise evaluation.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Online Evaluation\\u200b\\nWhereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='tipExplore our videos on online evaluation:\\nOnline evaluation in our LangSmith Evaluation series\\nOnline evaluation with focus on guardrails in our LangSmith Evaluation series\\nOnline evaluation with focus on RAG in our LangSmith Evaluation series'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Experiment Configurations\\u200b\\nLangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below.\\ntipSee documentation on using evaluate here.\\nRepetitions\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='tip\\nSee the video on Repetitions in our LangSmith Evaluation series\\nSee our documentation on Repetitions'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Evaluating Specific LLM Applications\\nBelow, we will discuss evaluation of a few specific, popular LLM applications.\\nAgents\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Agents\\u200b\\nLLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"This sets up three general types of agent evaluations that users are often interested in:\\n\\nFinal Response: Evaluate the agent's final response.\\nSingle step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).\\nTrajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.\\nNote that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!\\nEvaluating an agent's final response\\u200b\\nOne way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time.\\nThe output should be the agent's final response.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"The output should be the agent's final response.\\nThe evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics.\\ntipSee our tutorial on evaluating agent response.\\nEvaluating a single step of an agent\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Evaluating a single step of an agent\\u200b\\nAgents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps.\\nThe outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristic evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging,\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses).\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"tipSee our tutorial on evaluating a single step of an agent.\\nEvaluating an agent's trajectory\\u200b\\nEvaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps.\\nThe inputs are again the inputs to the overall agent (the user input, and optionally a list of tools).\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='The outputs are a list of tool calls, which can be formulated as an \"exact\" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order).'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='The evaluator here is some function over the steps taken. Assessing the \"exact\" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='To address these flaws, evaluation metrics can focused on the number of \"incorrect\" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"tipSee our tutorial on evaluating agent trajectory.\\nBest practices\\u200b\\nAgents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects:\\ntip\\nTest multiple tool calling LLMs with your agent.\\nIt's possible that faster and / or lower cost LLMs show acceptable performance for your application.\\nTrying evaluating the agent at multiple levels - both end-to-end, as well as at particular steps\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Use repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.\\nSee the video on Repetitions in our LangSmith Evaluation series'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"Retrieval Augmented Generation (RAG)\\u200b\\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\\nDataset\\u200b\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Dataset\\u200b\\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\\nEvaluator\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"Evaluator\\u200b\\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"When evaluating RAG applications, you have two main options:\\n\\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='tipDive deeper into RAG evaluation concepts with our LangSmith video series:\\nRAG answer correctness evaluation\\nRAG answer hallucination\\nRAG document relevance\\nRAG intermediate steps evaluation\\n\\nApplying RAG Evaluation\\u200b\\nWhen applying RAG evaluation, consider the following approaches:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\\n\\n\\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\\n\\n\\ntipExplore our LangSmith video series for more insights on RAG evaluation:\\nRAG with online evaluation\\nRAG pairwise evaluation'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='RAG evaluation summary\\u200b\\nUse CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes\\nCommon use-cases\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Common use-cases\\u200b\\nSummarization\\u200b\\nSummarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.\\nDeveloper curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs):\\ntipSee our LangSmith video series to go deeper on these concepts:\\nVideo on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes\\nClassification / Tagging\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Classification / Tagging\\u200b\\nClassification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='label (e.g., using metrics such as precision, recall, etc).'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference).\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc).\\ntipSee our LangSmith video series to go deeper on these concepts:\\nOnline evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNoWas this page helpful?You can leave detailed feedback on GitHub.PreviousAdd observability to your LLM applicationNextConceptual GuideDatasetsCreating datasetsTypes of datasetsPartitioning'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content=\"datasetsTypes of datasetsPartitioning datasetsEvaluatorsHumanHeuristicLLM-as-judgePairwiseApplying evaluationsUnit TestsRegression TestingBack-testingPairwise-testingOnline EvaluationExperiment ConfigurationsAgentsEvaluating an agent's final responseEvaluating a single step of an agentEvaluating an agent's trajectoryBest practicesRetrieval Augmented Generation (RAG)DatasetEvaluatorApplying RAG EvaluationRAG evaluation summaryCommon use-casesSummarizationClassification /\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.', 'language': 'en'}, page_content='use-casesSummarizationClassification / TaggingCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap= 50)\n",
    "text_chunks = splitter.split_documents(docs)\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store_db =FAISS.from_documents(text_chunks, embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Once you have an application in production, you start getting valuable information: \"\n",
    "results= vector_store_db.similarity_search(query)\n",
    "results[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based on the context:\n",
    "<context>\n",
    "\n",
    "{context}\n",
    "\n",
    "</context>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "response_0 =document_chain.invoke({\"input\": \"If your application is going well, you will likely get a lot of usage!\",\n",
    "                                 \"context\": [Document(page_content=\"If your application is going well, you will likely get a lot of usage!\")]\n",
    "                                 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retriever \u001b[38;5;241m=\u001b[39m\u001b[43mvector_store_db\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain\n\u001b[0;32m      3\u001b[0m retrieval_chain\u001b[38;5;241m=\u001b[39mcreate_retrieval_chain(retriever, document_chain)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_store_db' is not defined"
     ]
    }
   ],
   "source": [
    "retriever =vector_store_db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever, document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =retrieval_chain.invoke({\"input\": \"If your application is going well, you will likely get a lot of usage!\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
